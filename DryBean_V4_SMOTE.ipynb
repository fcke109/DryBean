{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI218 Group Project: Dry Bean Classification\n",
    "## V3 — SMOTE + PCA + GridSearchCV + Advanced Models\n",
    "\n",
    "**Enhancements over V2:**\n",
    "1. **SMOTE** — Synthetic Minority Over-sampling via `imblearn` Pipeline (no data leakage)\n",
    "2. **PCA** — Dimensionality reduction to remove correlated features\n",
    "3. **GridSearchCV** — Systematic hyperparameter tuning for SVM (C, gamma) and KNN (k)\n",
    "4. **Model Expansion** — XGBoost, LightGBM, and MLP (Neural Network) for complex non-linear patterns\n",
    "5. **Macro-F1** as primary metric for fair 7-class evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ucimlrepo imbalanced-learn xgboost lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Use imblearn Pipeline (not sklearn) so SMOTE runs inside each CV fold\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, precision_score, recall_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Configuration\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
    "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "N_SPLITS = 5\n",
    "\n",
    "print(\"Imports and configuration complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = None, None\n",
    "\n",
    "try:\n",
    "    from ucimlrepo import fetch_ucirepo\n",
    "    print(\"Loading Dry Bean dataset from UCI ML Repository...\")\n",
    "    dataset = fetch_ucirepo(id=602)\n",
    "    X = dataset.data.features\n",
    "    y = dataset.data.targets.values.ravel()\n",
    "    print(\"Loaded successfully via ucimlrepo.\")\n",
    "except Exception:\n",
    "    local_path = os.path.join(BASE_DIR, \"Dry_Bean_Dataset.csv\")\n",
    "    if os.path.exists(local_path):\n",
    "        print(f\"Loading from local CSV: {local_path}\")\n",
    "        df = pd.read_csv(local_path)\n",
    "        X = df.iloc[:, :-1]\n",
    "        y = df.iloc[:, -1].values\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            \"Could not load dataset via ucimlrepo and local CSV not found.\\n\"\n",
    "            \"Place Dry_Bean_Dataset.csv next to this notebook, or install ucimlrepo.\"\n",
    "        )\n",
    "\n",
    "print(f\"Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
    "print(f\"Classes: {np.unique(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Basic Checks & Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values\n",
    "missing = int(pd.DataFrame(X).isnull().sum().sum())\n",
    "print(f\"Missing values: {missing}\")\n",
    "if missing > 0:\n",
    "    X = pd.DataFrame(X).fillna(pd.DataFrame(X).median())\n",
    "    print(\"Filled missing values with median.\")\n",
    "\n",
    "# Class distribution\n",
    "class_counts_before = pd.Series(y).value_counts().sort_index()\n",
    "print(\"\\nClass distribution (before SMOTE):\")\n",
    "for cls, cnt in class_counts_before.items():\n",
    "    print(f\"  {cls}: {cnt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Encode Labels & Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "class_names = le.classes_\n",
    "print(f\"Encoded classes: {dict(zip(class_names, range(len(class_names))))}\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "print(f\"Train: {X_train.shape[0]} samples\")\n",
    "print(f\"Test:  {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Address Imbalance — SMOTE Visualisation\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique) generates synthetic samples for minority classes like **BOMBAY** (only 522 samples vs 3546 for DERMASON). Inside the pipeline, SMOTE is applied only to training folds — the test set remains untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote_vis = SMOTE(random_state=RANDOM_STATE)\n",
    "scaler_vis = StandardScaler()\n",
    "X_train_scaled_vis = scaler_vis.fit_transform(X_train)\n",
    "X_train_smote_vis, y_train_smote_vis = smote_vis.fit_resample(X_train_scaled_vis, y_train)\n",
    "\n",
    "counts_before = pd.Series(y_train).value_counts().sort_index()\n",
    "counts_after  = pd.Series(y_train_smote_vis).value_counts().sort_index()\n",
    "labels = [class_names[i] for i in counts_before.index]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "axes[0].bar(labels, counts_before.values, edgecolor=\"black\", color=\"steelblue\")\n",
    "axes[0].set_title(\"Class Distribution \\u2014 Before SMOTE (Train Set)\")\n",
    "axes[0].set_xlabel(\"Bean Class\"); axes[0].set_ylabel(\"Count\")\n",
    "axes[0].tick_params(axis=\"x\", rotation=30)\n",
    "\n",
    "axes[1].bar(labels, counts_after.values, edgecolor=\"black\", color=\"seagreen\")\n",
    "axes[1].set_title(\"Class Distribution \\u2014 After SMOTE (Train Set)\")\n",
    "axes[1].set_xlabel(\"Bean Class\"); axes[1].set_ylabel(\"Count\")\n",
    "axes[1].tick_params(axis=\"x\", rotation=30)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"smote_class_distribution.png\"), dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Training samples before SMOTE: {X_train.shape[0]}\")\n",
    "print(f\"Training samples after  SMOTE: {X_train_smote_vis.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Advanced Feature Engineering — PCA (Dimensionality Reduction)\n",
    "\n",
    "Many of the 16 features are highly correlated (e.g. Area vs Perimeter, MajorAxisLength vs MinorAxisLength). PCA projects the data onto orthogonal principal components, removing redundancy while retaining >= 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit PCA on all components to analyse variance\n",
    "pca_full = PCA(random_state=RANDOM_STATE)\n",
    "pca_full.fit(X_train_scaled_vis)  # fit on scaled (non-SMOTE) training data\n",
    "\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "n_components_95 = int(np.argmax(cumulative_variance >= 0.95) + 1)\n",
    "\n",
    "print(f\"Original features:          {X_train.shape[1]}\")\n",
    "print(f\"Components for 95% variance: {n_components_95}\")\n",
    "print(f\"Variance retained:          {cumulative_variance[n_components_95 - 1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explained variance plot\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.bar(range(1, len(pca_full.explained_variance_ratio_) + 1),\n",
    "       pca_full.explained_variance_ratio_, alpha=0.6,\n",
    "       label=\"Individual\", color=\"steelblue\", edgecolor=\"black\")\n",
    "ax.step(range(1, len(cumulative_variance) + 1), cumulative_variance,\n",
    "        where=\"mid\", label=\"Cumulative\", color=\"darkorange\", linewidth=2)\n",
    "ax.axhline(y=0.95, color=\"red\", linestyle=\"--\", label=\"95% threshold\")\n",
    "ax.axvline(x=n_components_95, color=\"green\", linestyle=\"--\", alpha=0.7,\n",
    "           label=f\"n_components = {n_components_95}\")\n",
    "ax.set_xlabel(\"Principal Component\")\n",
    "ax.set_ylabel(\"Explained Variance Ratio\")\n",
    "ax.set_title(\"PCA \\u2014 Explained Variance Analysis\")\n",
    "ax.legend()\n",
    "ax.set_xticks(range(1, len(pca_full.explained_variance_ratio_) + 1))\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"pca_explained_variance.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation heatmap (before PCA) — shows why PCA helps\n",
    "corr_matrix = pd.DataFrame(X_train_scaled_vis, columns=X.columns).corr()\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "im = ax.imshow(corr_matrix.values, cmap=\"coolwarm\", vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(len(X.columns)))\n",
    "ax.set_yticks(range(len(X.columns)))\n",
    "ax.set_xticklabels(X.columns, rotation=45, ha=\"right\", fontsize=8)\n",
    "ax.set_yticklabels(X.columns, fontsize=8)\n",
    "ax.set_title(\"Feature Correlation Matrix (before PCA)\")\n",
    "plt.colorbar(im, ax=ax)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"correlation_matrix_before_pca.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Systematic Optimisation — GridSearchCV\n",
    "\n",
    "We use `GridSearchCV` with `imblearn.Pipeline` (SMOTE inside each CV fold) to find:\n",
    "- **SVM**: optimal `C` and `gamma`\n",
    "- **KNN**: optimal `k` (n_neighbors) and weighting scheme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# --- SVM GridSearch ---\n",
    "print(\"SVM GridSearchCV (C, gamma)...\")\n",
    "svm_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\",    PCA(n_components=n_components_95, random_state=RANDOM_STATE)),\n",
    "    (\"smote\",  SMOTE(random_state=RANDOM_STATE)),\n",
    "    (\"clf\",    SVC(kernel=\"rbf\", random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "svm_param_grid = {\n",
    "    \"clf__C\":     [0.1, 1, 10, 100],\n",
    "    \"clf__gamma\": [\"scale\", \"auto\", 0.01, 0.1]\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(\n",
    "    svm_pipeline, svm_param_grid,\n",
    "    cv=cv, scoring=\"f1_macro\", n_jobs=-1, verbose=0\n",
    ")\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"  Best params:      {svm_grid.best_params_}\")\n",
    "print(f\"  Best CV macro-F1: {svm_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- KNN GridSearch ---\n",
    "print(\"KNN GridSearchCV (n_neighbors, weights)...\")\n",
    "knn_pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"pca\",    PCA(n_components=n_components_95, random_state=RANDOM_STATE)),\n",
    "    (\"smote\",  SMOTE(random_state=RANDOM_STATE)),\n",
    "    (\"clf\",    KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "knn_param_grid = {\n",
    "    \"clf__n_neighbors\": [3, 5, 7, 9, 11, 15, 21],\n",
    "    \"clf__weights\":     [\"uniform\", \"distance\"]\n",
    "}\n",
    "\n",
    "knn_grid = GridSearchCV(\n",
    "    knn_pipeline, knn_param_grid,\n",
    "    cv=cv, scoring=\"f1_macro\", n_jobs=-1, verbose=0\n",
    ")\n",
    "knn_grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"  Best params:      {knn_grid.best_params_}\")\n",
    "print(f\"  Best CV macro-F1: {knn_grid.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch results visualisation\n",
    "svm_results = pd.DataFrame(svm_grid.cv_results_)\n",
    "C_values = svm_param_grid[\"clf__C\"]\n",
    "gamma_values = [str(g) for g in svm_param_grid[\"clf__gamma\"]]\n",
    "scores_matrix = np.zeros((len(C_values), len(gamma_values)))\n",
    "for idx, row in svm_results.iterrows():\n",
    "    c_idx = C_values.index(row[\"param_clf__C\"])\n",
    "    g_idx = gamma_values.index(str(row[\"param_clf__gamma\"]))\n",
    "    scores_matrix[c_idx, g_idx] = row[\"mean_test_score\"]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# SVM heatmap\n",
    "im0 = axes[0].imshow(scores_matrix, cmap=\"YlOrRd\", aspect=\"auto\")\n",
    "axes[0].set_xticks(range(len(gamma_values)))\n",
    "axes[0].set_yticks(range(len(C_values)))\n",
    "axes[0].set_xticklabels(gamma_values)\n",
    "axes[0].set_yticklabels(C_values)\n",
    "axes[0].set_xlabel(\"gamma\"); axes[0].set_ylabel(\"C\")\n",
    "axes[0].set_title(\"SVM GridSearchCV \\u2014 macro-F1 Scores\")\n",
    "for i in range(len(C_values)):\n",
    "    for j in range(len(gamma_values)):\n",
    "        axes[0].text(j, i, f\"{scores_matrix[i, j]:.3f}\",\n",
    "                     ha=\"center\", va=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "# KNN line plot\n",
    "knn_results = pd.DataFrame(knn_grid.cv_results_)\n",
    "for weight in [\"uniform\", \"distance\"]:\n",
    "    mask = knn_results[\"param_clf__weights\"] == weight\n",
    "    subset = knn_results[mask].sort_values(\"param_clf__n_neighbors\")\n",
    "    axes[1].plot(subset[\"param_clf__n_neighbors\"], subset[\"mean_test_score\"],\n",
    "                 marker=\"o\", label=f\"weights={weight}\", linewidth=2)\n",
    "    axes[1].fill_between(\n",
    "        subset[\"param_clf__n_neighbors\"],\n",
    "        subset[\"mean_test_score\"] - subset[\"std_test_score\"],\n",
    "        subset[\"mean_test_score\"] + subset[\"std_test_score\"],\n",
    "        alpha=0.15\n",
    "    )\n",
    "axes[1].set_xlabel(\"k (n_neighbors)\"); axes[1].set_ylabel(\"CV macro-F1\")\n",
    "axes[1].set_title(\"KNN GridSearchCV \\u2014 macro-F1 vs k\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"gridsearch_results.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Model Training — All Models (PCA + SMOTE + Tuned Hyperparameters)\n",
    "\n",
    "We train 7 models total:\n",
    "1. **Logistic Regression** — linear baseline\n",
    "2. **SVM (RBF, tuned)** — best C/gamma from GridSearchCV\n",
    "3. **KNN (tuned)** — best k/weights from GridSearchCV\n",
    "4. **Random Forest** — ensemble of decision trees\n",
    "5. **XGBoost** — gradient boosting\n",
    "6. **LightGBM** — fast gradient boosting\n",
    "7. **MLP (Neural Network)** — deep learning for non-linear patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svm_C     = svm_grid.best_params_[\"clf__C\"]\n",
    "best_svm_gamma = svm_grid.best_params_[\"clf__gamma\"]\n",
    "best_knn_k     = knn_grid.best_params_[\"clf__n_neighbors\"]\n",
    "best_knn_w     = knn_grid.best_params_[\"clf__weights\"]\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\",    PCA(n_components=n_components_95, random_state=RANDOM_STATE)),\n",
    "        (\"smote\",  SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"clf\",    LogisticRegression(max_iter=3000, random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    \"SVM (RBF, tuned)\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\",    PCA(n_components=n_components_95, random_state=RANDOM_STATE)),\n",
    "        (\"smote\",  SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"clf\",    SVC(kernel=\"rbf\", C=best_svm_C, gamma=best_svm_gamma,\n",
    "                       random_state=RANDOM_STATE))\n",
    "    ]),\n",
    "    f\"KNN (k={best_knn_k}, tuned)\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\",    PCA(n_components=n_components_95, random_state=RANDOM_STATE)),\n",
    "        (\"smote\",  SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"clf\",    KNeighborsClassifier(n_neighbors=best_knn_k, weights=best_knn_w))\n",
    "    ]),\n",
    "    \"Random Forest\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\",    PCA(n_components=n_components_95, random_state=RANDOM_STATE)),\n",
    "        (\"smote\",  SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"clf\",    RandomForestClassifier(\n",
    "                       n_estimators=300,\n",
    "                       class_weight=\"balanced\",\n",
    "                       random_state=RANDOM_STATE,\n",
    "                       n_jobs=-1\n",
    "                   ))\n",
    "    ]),\n",
    "    \"XGBoost\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\",    PCA(n_components=n_components_95, random_state=RANDOM_STATE)),\n",
    "        (\"smote\",  SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"clf\",    XGBClassifier(\n",
    "                       n_estimators=300,\n",
    "                       learning_rate=0.1,\n",
    "                       max_depth=6,\n",
    "                       use_label_encoder=False,\n",
    "                       eval_metric=\"mlogloss\",\n",
    "                       random_state=RANDOM_STATE,\n",
    "                       n_jobs=-1\n",
    "                   ))\n",
    "    ]),\n",
    "    \"LightGBM\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\",    PCA(n_components=n_components_95, random_state=RANDOM_STATE)),\n",
    "        (\"smote\",  SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"clf\",    LGBMClassifier(\n",
    "                       n_estimators=300,\n",
    "                       learning_rate=0.1,\n",
    "                       max_depth=6,\n",
    "                       class_weight=\"balanced\",\n",
    "                       random_state=RANDOM_STATE,\n",
    "                       n_jobs=-1,\n",
    "                       verbose=-1\n",
    "                   ))\n",
    "    ]),\n",
    "    \"MLP (Neural Network)\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pca\",    PCA(n_components=n_components_95, random_state=RANDOM_STATE)),\n",
    "        (\"smote\",  SMOTE(random_state=RANDOM_STATE)),\n",
    "        (\"clf\",    MLPClassifier(\n",
    "                       hidden_layer_sizes=(128, 64, 32),\n",
    "                       activation=\"relu\",\n",
    "                       solver=\"adam\",\n",
    "                       max_iter=500,\n",
    "                       early_stopping=True,\n",
    "                       validation_fraction=0.1,\n",
    "                       random_state=RANDOM_STATE\n",
    "                   ))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "print(f\"Models to train: {len(models)}\")\n",
    "for name in models:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining: {name}\")\n",
    "\n",
    "    # CV score on TRAIN ONLY — SMOTE applied inside each fold automatically\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"f1_macro\")\n",
    "\n",
    "    # Fit on full training set, evaluate on untouched test set\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    acc          = accuracy_score(y_test, y_pred)\n",
    "    f1_macro     = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    f1_weighted  = f1_score(y_test, y_pred, average=\"weighted\")\n",
    "    prec_macro   = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec_macro    = recall_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    results[name] = {\n",
    "        \"cv_f1_macro_mean\":     float(cv_scores.mean()),\n",
    "        \"cv_f1_macro_std\":      float(cv_scores.std()),\n",
    "        \"test_accuracy\":        float(acc),\n",
    "        \"test_f1_macro\":        float(f1_macro),\n",
    "        \"test_f1_weighted\":     float(f1_weighted),\n",
    "        \"test_precision_macro\": float(prec_macro),\n",
    "        \"test_recall_macro\":    float(rec_macro),\n",
    "        \"y_pred\":               y_pred,\n",
    "        \"model\":                model\n",
    "    }\n",
    "\n",
    "    print(f\"  CV macro-F1:        {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print(f\"  Test Accuracy:      {acc:.4f}\")\n",
    "    print(f\"  Test F1 (macro):    {f1_macro:.4f}\")\n",
    "    print(f\"  Test F1 (weighted): {f1_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model selection\n",
    "best_model_name = max(results, key=lambda k: results[k][\"cv_f1_macro_mean\"])\n",
    "best = results[best_model_name]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"BEST MODEL (by CV macro-F1): {best_model_name}\")\n",
    "print(f\"CV macro-F1:   {best['cv_f1_macro_mean']:.4f}\")\n",
    "print(f\"Test Accuracy: {best['test_accuracy']:.4f}\")\n",
    "print(f\"Test macro-F1: {best['test_f1_macro']:.4f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Confusion Matrices — All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_models = len(results)\n",
    "n_cols = 4\n",
    "n_rows = (n_models + n_cols - 1) // n_cols\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(7 * n_cols, 6 * n_rows))\n",
    "axes_flat = axes.flatten() if n_models > 1 else [axes]\n",
    "\n",
    "for idx, (name, r) in enumerate(results.items()):\n",
    "    ax = axes_flat[idx]\n",
    "    cm = confusion_matrix(y_test, r[\"y_pred\"])\n",
    "    im = ax.imshow(cm, cmap=\"Blues\")\n",
    "    ax.set_title(f\"{name}\", fontsize=10, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Predicted\"); ax.set_ylabel(\"Actual\")\n",
    "    ax.set_xticks(range(len(class_names)))\n",
    "    ax.set_yticks(range(len(class_names)))\n",
    "    ax.set_xticklabels(class_names, rotation=30, ha=\"right\", fontsize=7)\n",
    "    ax.set_yticklabels(class_names, fontsize=7)\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", fontsize=6,\n",
    "                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(n_models, len(axes_flat)):\n",
    "    axes_flat[idx].set_visible(False)\n",
    "\n",
    "plt.suptitle(\"Confusion Matrices \\u2014 All Models (PCA + SMOTE + Tuned)\",\n",
    "             fontsize=14, fontweight=\"bold\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrices_all.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Classification Report (Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, best[\"y_pred\"], target_names=class_names, zero_division=0)\n",
    "\n",
    "print(f\"Best Model: {best_model_name}\\n\")\n",
    "print(report)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, \"classification_report_best.txt\"), \"w\") as f:\n",
    "    f.write(f\"Best Model (by CV macro-F1): {best_model_name}\\n\")\n",
    "    f.write(f\"CV macro-F1 mean: {best['cv_f1_macro_mean']:.4f} (std {best['cv_f1_macro_std']:.4f})\\n\")\n",
    "    f.write(f\"Test Accuracy: {best['test_accuracy']:.4f}\\n\")\n",
    "    f.write(f\"Test macro-F1: {best['test_f1_macro']:.4f}\\n\")\n",
    "    f.write(f\"Test weighted-F1: {best['test_f1_weighted']:.4f}\\n\\n\")\n",
    "    f.write(report)\n",
    "print(\"Saved: classification_report_best.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Model Comparison Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names  = list(results.keys())\n",
    "cv_means     = [results[n][\"cv_f1_macro_mean\"] for n in model_names]\n",
    "test_f1      = [results[n][\"test_f1_macro\"]    for n in model_names]\n",
    "test_acc     = [results[n][\"test_accuracy\"]     for n in model_names]\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars1 = ax.bar(x - width, cv_means, width, label=\"CV macro-F1 (mean)\",\n",
    "               color=\"steelblue\", edgecolor=\"black\")\n",
    "bars2 = ax.bar(x,         test_f1,  width, label=\"Test macro-F1\",\n",
    "               color=\"seagreen\", edgecolor=\"black\")\n",
    "bars3 = ax.bar(x + width, test_acc, width, label=\"Test Accuracy\",\n",
    "               color=\"darkorange\", edgecolor=\"black\")\n",
    "\n",
    "ax.set_title(\"Model Comparison (PCA + SMOTE + GridSearchCV)\",\n",
    "             fontsize=13, fontweight=\"bold\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_names, fontsize=8, rotation=15, ha=\"right\")\n",
    "ax.set_ylim(0.85, 1.01)\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "for bar in [*bars1, *bars2, *bars3]:\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.001,\n",
    "            f\"{bar.get_height():.3f}\", ha=\"center\", va=\"bottom\", fontsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, \"model_comparison.png\"), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame([\n",
    "    {\n",
    "        \"Model\":                name,\n",
    "        \"CV_MacroF1_Mean\":      r[\"cv_f1_macro_mean\"],\n",
    "        \"CV_MacroF1_Std\":       r[\"cv_f1_macro_std\"],\n",
    "        \"Test_Accuracy\":        r[\"test_accuracy\"],\n",
    "        \"Test_Precision_Macro\": r[\"test_precision_macro\"],\n",
    "        \"Test_Recall_Macro\":    r[\"test_recall_macro\"],\n",
    "        \"Test_F1_Macro\":        r[\"test_f1_macro\"],\n",
    "        \"Test_F1_Weighted\":     r[\"test_f1_weighted\"],\n",
    "    }\n",
    "    for name, r in results.items()\n",
    "])\n",
    "\n",
    "summary_csv_path = os.path.join(OUTPUT_DIR, \"results_summary_v3.csv\")\n",
    "summary_df.to_csv(summary_csv_path, index=False)\n",
    "\n",
    "# Display table\n",
    "display(summary_df.style.highlight_max(\n",
    "    subset=[\"CV_MacroF1_Mean\", \"Test_Accuracy\", \"Test_F1_Macro\"],\n",
    "    color=\"lightgreen\"\n",
    "))\n",
    "print(f\"\\nSaved: results_summary_v3.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Enhancement Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ENHANCEMENT SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n1. SMOTE (Address Imbalance):\")\n",
    "print(f\"   Augmented minority classes (e.g. BOMBAY {counts_before.min()} -> {counts_after.max()} samples)\")\n",
    "print(f\"   Applied inside imblearn Pipeline — no data leakage\")\n",
    "\n",
    "print(f\"\\n2. PCA (Feature Engineering):\")\n",
    "print(f\"   Reduced {X_train.shape[1]} correlated features -> {n_components_95} orthogonal components\")\n",
    "print(f\"   Variance retained: {cumulative_variance[n_components_95-1]*100:.1f}%\")\n",
    "\n",
    "print(f\"\\n3. GridSearchCV (Systematic Optimisation):\")\n",
    "print(f\"   SVM:  C={best_svm_C}, gamma={best_svm_gamma}\")\n",
    "print(f\"   KNN:  k={best_knn_k}, weights={best_knn_w}\")\n",
    "\n",
    "print(f\"\\n4. Model Expansion (Advanced Models):\")\n",
    "print(f\"   Added XGBoost, LightGBM, and MLP (Neural Network)\")\n",
    "print(f\"   Total models evaluated: {len(results)}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"CV macro-F1:   {best['cv_f1_macro_mean']:.4f}\")\n",
    "print(f\"Test macro-F1: {best['test_f1_macro']:.4f}\")\n",
    "print(f\"Test Accuracy: {best['test_accuracy']:.4f}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nAll outputs saved to: {OUTPUT_DIR}\")\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}