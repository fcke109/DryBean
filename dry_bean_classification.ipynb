{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSCI218 Group Project: Dry Bean Classification\n",
    "\n",
    "**University of Wollongong - SIM Session 1, 2026**\n",
    "\n",
    "This notebook performs multi-class classification of dry bean varieties using machine learning algorithms.\n",
    "\n",
    "**Dataset:** UCI Dry Bean Dataset (13,611 samples, 16 features, 7 classes)  \n",
    "**Models:** K-Nearest Neighbours, Random Forest, Support Vector Machine (SVM)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "We load the Dry Bean Dataset from the UCI Machine Learning Repository. The dataset contains 13,611 samples of 7 dry bean varieties with 16 features extracted from grain images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from UCI ML Repository\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "print(\"Loading Dry Bean dataset from UCI ML Repository...\")\n",
    "dataset = fetch_ucirepo(id=602)\n",
    "\n",
    "X = dataset.data.features\n",
    "y = dataset.data.targets.values.ravel()\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Samples: {X.shape[0]}\")\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"\\nBean classes: {np.unique(y)}\")\n",
    "print(f\"\\nFeature names: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset info\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Exploratory Data Analysis (EDA)\n",
    "\n",
    "Let's explore the dataset to understand the class distribution, feature correlations, and data characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "class_counts = pd.Series(y).value_counts().sort_index()\n",
    "print(\"Class Distribution:\")\n",
    "print(class_counts)\n",
    "print(f\"\\nTotal samples: {class_counts.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot class distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = sns.color_palette(\"husl\", len(class_counts))\n",
    "bars = ax.bar(class_counts.index, class_counts.values, color=colors, edgecolor='black')\n",
    "ax.set_xlabel(\"Bean Type\", fontsize=13)\n",
    "ax.set_ylabel(\"Number of Samples\", fontsize=13)\n",
    "ax.set_title(\"Class Distribution in Dry Bean Dataset\", fontsize=15, fontweight='bold')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, class_counts.values):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 50,\n",
    "            str(val), ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Feature Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation heatmap\n",
    "fig, ax = plt.subplots(figsize=(14, 11))\n",
    "corr = X.corr()\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap=\"RdBu_r\",\n",
    "            center=0, linewidths=0.5, ax=ax, annot_kws={\"size\": 7})\n",
    "ax.set_title(\"Feature Correlation Heatmap\", fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Feature Distributions by Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature distributions by class\n",
    "key_features = ['Area', 'Perimeter', 'Roundness', 'Compactness']\n",
    "available_features = [f for f in key_features if f in X.columns]\n",
    "if len(available_features) < 4:\n",
    "    available_features = list(X.columns[:4])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "for idx, feat in enumerate(available_features[:4]):\n",
    "    ax = axes[idx // 2][idx % 2]\n",
    "    for cls in np.unique(y):\n",
    "        subset = X[pd.Series(y) == cls][feat]\n",
    "        ax.hist(subset, bins=30, alpha=0.5, label=cls, density=True)\n",
    "    ax.set_title(f\"Distribution of {feat}\", fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel(feat)\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend(fontsize=7, loc='upper right')\n",
    "\n",
    "plt.suptitle(\"Feature Distributions by Bean Type\", fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Boxplots for Key Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots for key features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "df_plot = X.copy()\n",
    "df_plot['Class'] = y\n",
    "\n",
    "for idx, feat in enumerate(available_features[:4]):\n",
    "    ax = axes[idx // 2][idx % 2]\n",
    "    sns.boxplot(data=df_plot, x='Class', y=feat, ax=ax, palette=\"husl\")\n",
    "    ax.set_title(f\"Boxplot of {feat}\", fontsize=12, fontweight='bold')\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "\n",
    "plt.suptitle(\"Feature Boxplots by Bean Type\", fontsize=15, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "\n",
    "Steps:\n",
    "1. Encode target labels (string to integer)\n",
    "2. Check for missing values\n",
    "3. Split data into training and test sets (80/20, stratified)\n",
    "4. Standardize features (zero mean, unit variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode target labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "class_names = le.classes_\n",
    "\n",
    "print(\"Label Encoding:\")\n",
    "for i, cls in enumerate(class_names):\n",
    "    print(f\"  {cls} -> {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "missing = X.isnull().sum().sum()\n",
    "print(f\"Missing values: {missing}\")\n",
    "\n",
    "if missing > 0:\n",
    "    X = X.fillna(X.median())\n",
    "    print(\"Filled missing values with median.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test split (80/20, stratified)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE, \n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({100*(1-TEST_SIZE):.0f}%)\")\n",
    "print(f\"Test set:     {X_test.shape[0]} samples ({100*TEST_SIZE:.0f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features standardized (zero mean, unit variance).\")\n",
    "print(f\"\\nTraining set mean (should be ~0): {X_train_scaled.mean(axis=0).round(2)}\")\n",
    "print(f\"Training set std (should be ~1):  {X_train_scaled.std(axis=0).round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training & Evaluation\n",
    "\n",
    "We train and evaluate 3 machine learning models:\n",
    "1. **K-Nearest Neighbours (KNN)** - Instance-based learning, k=5\n",
    "2. **Random Forest** - Ensemble of 100 decision trees\n",
    "3. **SVM (RBF kernel)** - Support Vector Machine with radial basis function kernel\n",
    "\n",
    "Each model is evaluated using:\n",
    "- 5-fold stratified cross-validation on training set\n",
    "- Final evaluation on held-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models\n",
    "models = {\n",
    "    \"K-Nearest Neighbours\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    \"SVM (RBF)\": SVC(kernel='rbf', C=10, gamma='scale', random_state=RANDOM_STATE),\n",
    "}\n",
    "\n",
    "print(\"Models to train:\")\n",
    "for name in models:\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "results = {}\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Cross-validation on training set\n",
    "    cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    # Fit on full training set, predict on test set\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    prec = precision_score(y_test, y_pred, average='weighted')\n",
    "    rec = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    \n",
    "    # Store results\n",
    "    results[name] = {\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'accuracy': acc,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'f1': f1,\n",
    "        'y_pred': y_pred,\n",
    "        'model': model,\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n5-Fold CV Accuracy:  {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
    "    print(f\"Test Accuracy:       {acc:.4f}\")\n",
    "    print(f\"Test Precision (W):  {prec:.4f}\")\n",
    "    print(f\"Test Recall (W):     {rec:.4f}\")\n",
    "    print(f\"Test F1-Score (W):   {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Results Comparison & Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results summary DataFrame\n",
    "model_names = list(results.keys())\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': model_names,\n",
    "    'CV Accuracy': [f\"{results[m]['cv_mean']:.4f} +/- {results[m]['cv_std']:.4f}\" for m in model_names],\n",
    "    'Test Accuracy': [results[m]['accuracy'] for m in model_names],\n",
    "    'Precision': [results[m]['precision'] for m in model_names],\n",
    "    'Recall': [results[m]['recall'] for m in model_names],\n",
    "    'F1-Score': [results[m]['f1'] for m in model_names],\n",
    "})\n",
    "\n",
    "summary_df = summary_df.set_index('Model')\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model comparison bar chart\n",
    "metrics_data = {\n",
    "    'Accuracy': [results[m]['accuracy'] for m in model_names],\n",
    "    'Precision': [results[m]['precision'] for m in model_names],\n",
    "    'Recall': [results[m]['recall'] for m in model_names],\n",
    "    'F1-Score': [results[m]['f1'] for m in model_names],\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.18\n",
    "multiplier = 0\n",
    "colors_metrics = ['#2196F3', '#4CAF50', '#FF9800', '#F44336']\n",
    "\n",
    "for (metric, values), color in zip(metrics_data.items(), colors_metrics):\n",
    "    offset = width * multiplier\n",
    "    bars = ax.bar(x + offset, values, width, label=metric, color=color, edgecolor='black', linewidth=0.5)\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.003,\n",
    "                f\"{val:.3f}\", ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
    "    multiplier += 1\n",
    "\n",
    "ax.set_xlabel(\"Model\", fontsize=13)\n",
    "ax.set_ylabel(\"Score\", fontsize=13)\n",
    "ax.set_title(\"Model Performance Comparison on Dry Bean Dataset\", fontsize=15, fontweight='bold')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(model_names, fontsize=11)\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.set_ylim(0.85, 0.98)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Cross-Validation Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-validation comparison with error bars\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "cv_means = [results[m]['cv_mean'] for m in model_names]\n",
    "cv_stds = [results[m]['cv_std'] for m in model_names]\n",
    "\n",
    "bars = ax.bar(model_names, cv_means, yerr=cv_stds, capsize=8,\n",
    "              color=sns.color_palette(\"viridis\", len(model_names)), edgecolor='black')\n",
    "\n",
    "for bar, mean, std in zip(bars, cv_means, cv_stds):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + std + 0.005,\n",
    "            f\"{mean:.4f}\", ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel(\"Model\", fontsize=13)\n",
    "ax.set_ylabel(\"5-Fold CV Accuracy\", fontsize=13)\n",
    "ax.set_title(\"Cross-Validation Accuracy Comparison\", fontsize=15, fontweight='bold')\n",
    "ax.set_ylim(0.90, 0.96)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices for all models\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for idx, (name, res) in enumerate(results.items()):\n",
    "    ax = axes[idx]\n",
    "    cm = confusion_matrix(y_test, res['y_pred'])\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    ax.set_title(f\"{name}\\n(Accuracy: {res['accuracy']:.4f})\", fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"Actual\")\n",
    "    ax.tick_params(axis='x', rotation=30)\n",
    "    ax.tick_params(axis='y', rotation=0)\n",
    "\n",
    "plt.suptitle(\"Confusion Matrices for All Models\", fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Best Model - Detailed Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best model\n",
    "best_model_name = max(results, key=lambda m: results[m]['accuracy'])\n",
    "best_res = results[best_model_name]\n",
    "\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {best_res['accuracy']:.4f}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, best_res['y_pred'], target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Feature Importance (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from Random Forest\n",
    "rf_model = results['Random Forest']['model']\n",
    "importances = rf_model.feature_importances_\n",
    "feat_imp = pd.Series(importances, index=X.columns).sort_values(ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "feat_imp.plot(kind='barh', ax=ax, color=sns.color_palette(\"viridis\", len(feat_imp)), edgecolor='black')\n",
    "ax.set_xlabel(\"Feature Importance\", fontsize=13)\n",
    "ax.set_title(\"Random Forest Feature Importance\", fontsize=15, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop 5 Most Important Features:\")\n",
    "for feat, imp in feat_imp.sort_values(ascending=False).head(5).items():\n",
    "    print(f\"  {feat}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "### Summary of Results\n",
    "\n",
    "We compared 3 machine learning algorithms on the Dry Bean Dataset:\n",
    "\n",
    "| Model | CV Accuracy | Test Accuracy |\n",
    "|-------|-------------|---------------|\n",
    "| K-Nearest Neighbours | 92.33% | 91.66% |\n",
    "| Random Forest | 92.40% | 92.07% |\n",
    "| **SVM (RBF)** | **93.36%** | **92.43%** |\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Best Model:** SVM with RBF kernel achieved the highest test accuracy (92.43%)\n",
    "2. **All models performed well:** >91% accuracy, showing the dataset is suitable for ML classification\n",
    "3. **BOMBAY class:** Easiest to classify (100% precision/recall) due to distinctive larger size\n",
    "4. **SIRA class:** Most challenging (87% F1) - often confused with DERMASON and SEKER\n",
    "5. **Important features:** ShapeFactor4, ShapeFactor2, and Compactness are most discriminative\n",
    "\n",
    "### Future Improvements\n",
    "\n",
    "- Hyperparameter tuning via Grid Search\n",
    "- Feature selection/dimensionality reduction (PCA)\n",
    "- Try gradient boosting methods (XGBoost, LightGBM)\n",
    "- Address class imbalance with SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{'Model':<25} {'CV Acc':>12} {'Test Acc':>12} {'F1-Score':>12}\")\n",
    "print(\"-\"*60)\n",
    "for name in model_names:\n",
    "    r = results[name]\n",
    "    print(f\"{name:<25} {r['cv_mean']:>12.4f} {r['accuracy']:>12.4f} {r['f1']:>12.4f}\")\n",
    "print(\"-\"*60)\n",
    "print(f\"\\nBest Model: {best_model_name} with {best_res['accuracy']*100:.2f}% test accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
