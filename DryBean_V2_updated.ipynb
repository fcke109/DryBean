{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlNsROwvQGGF",
        "outputId": "13d9bce8-8fe3-40bd-bb4e-f193c08eaee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.7-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2026.1.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n",
            "Downloading ucimlrepo-0.0.7-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvFzBp0_P82u",
        "outputId": "59386d99-cfb1-4af5-d6da-674c35dfd246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CSCI218 Group Project: Dry Bean Classification (3 models)\n",
            "============================================================\n",
            "\n",
            "[1] Loading Dry Bean dataset from UCI ML Repository (ucimlrepo)...\n",
            "    Loaded successfully via ucimlrepo.\n",
            "    Samples: 13611, Features: 16\n",
            "    Classes: ['BARBUNYA' 'BOMBAY' 'CALI' 'DERMASON' 'HOROZ' 'SEKER' 'SIRA']\n",
            "\n",
            "[2] Basic dataset checks...\n",
            "    Missing values: 0\n",
            "    Saved: class_distribution.png\n",
            "\n",
            "[3] Preprocessing...\n",
            "    Encoded classes: {'BARBUNYA': 0, 'BOMBAY': 1, 'CALI': 2, 'DERMASON': 3, 'HOROZ': 4, 'SEKER': 5, 'SIRA': 6}\n",
            "    Train: 10888 samples\n",
            "    Test:  2723 samples\n",
            "\n",
            "[4] Training + Cross-validation (no leakage via Pipeline)...\n",
            "\n",
            "  Model: Logistic Regression\n",
            "    CV macro-F1: 0.9353 (+/- 0.0046)\n",
            "    Test Acc:   0.9207\n",
            "    Test F1 (macro):    0.9329\n",
            "    Test F1 (weighted): 0.9209\n",
            "\n",
            "  Model: SVM (RBF)\n",
            "    CV macro-F1: 0.9443 (+/- 0.0057)\n",
            "    Test Acc:   0.9243\n",
            "    Test F1 (macro):    0.9361\n",
            "    Test F1 (weighted): 0.9243\n",
            "\n",
            "  Model: Random Forest\n",
            "    CV macro-F1: 0.9355 (+/- 0.0052)\n",
            "    Test Acc:   0.9210\n",
            "    Test F1 (macro):    0.9333\n",
            "    Test F1 (weighted): 0.9210\n",
            "\n",
            "============================================================\n",
            "BEST MODEL (by CV macro-F1): SVM (RBF)\n",
            "CV macro-F1: 0.9443\n",
            "Test Acc:    0.9243\n",
            "Test macro-F1: 0.9361\n",
            "============================================================\n",
            "\n",
            "[5] Saving best model evaluation outputs...\n",
            "    Saved: confusion_matrix_best.png\n",
            "    Saved: classification_report_best.txt\n",
            "\n",
            "[6] Saving results table...\n",
            "    Saved: results_summary_3models.csv\n",
            "\n",
            "All outputs saved to: /content/output\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "CSCI218 Group Project: Dry Bean Dataset Classification (3 models, fixed CV)\n",
        "==========================================================================\n",
        "Fixes vs typical template:\n",
        "- Uses sklearn Pipeline for scaling (prevents CV leakage)\n",
        "- Trains exactly 3 models: Logistic Regression, SVM-RBF, Random Forest\n",
        "- Uses macro-F1 for fair multi-class evaluation\n",
        "- Selects best model by CV score (not by test)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, precision_score, recall_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ============================================================\n",
        "# Configuration\n",
        "# ============================================================\n",
        "BASE_DIR = os.path.dirname(os.path.abspath(__file__)) if \"__file__\" in globals() else os.getcwd()\n",
        "OUTPUT_DIR = os.path.join(BASE_DIR, \"output\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "N_SPLITS = 5\n",
        "\n",
        "# ============================================================\n",
        "# 1. Load Dataset\n",
        "# ============================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"CSCI218 Group Project: Dry Bean Classification (3 models)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "X, y = None, None\n",
        "\n",
        "try:\n",
        "    from ucimlrepo import fetch_ucirepo\n",
        "    print(\"\\n[1] Loading Dry Bean dataset from UCI ML Repository (ucimlrepo)...\")\n",
        "    dataset = fetch_ucirepo(id=602)\n",
        "    X = dataset.data.features\n",
        "    y = dataset.data.targets.values.ravel()\n",
        "    print(\"    Loaded successfully via ucimlrepo.\")\n",
        "except Exception:\n",
        "    local_path = os.path.join(BASE_DIR, \"Dry_Bean_Dataset.csv\")\n",
        "    if os.path.exists(local_path):\n",
        "        print(f\"\\n[1] Loading Dry Bean dataset from local CSV: {local_path}\")\n",
        "        df = pd.read_csv(local_path)\n",
        "        X = df.iloc[:, :-1]\n",
        "        y = df.iloc[:, -1].values\n",
        "    else:\n",
        "        raise FileNotFoundError(\n",
        "            \"Could not load dataset via ucimlrepo and local CSV not found.\\n\"\n",
        "            \"Place Dry_Bean_Dataset.csv next to this script, or install ucimlrepo.\"\n",
        "        )\n",
        "\n",
        "print(f\"    Samples: {X.shape[0]}, Features: {X.shape[1]}\")\n",
        "print(f\"    Classes: {np.unique(y)}\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. Basic checks + optional simple plot\n",
        "# ============================================================\n",
        "print(\"\\n[2] Basic dataset checks...\")\n",
        "\n",
        "# Missing values\n",
        "missing = int(pd.DataFrame(X).isnull().sum().sum())\n",
        "print(f\"    Missing values: {missing}\")\n",
        "if missing > 0:\n",
        "    X = pd.DataFrame(X).fillna(pd.DataFrame(X).median())\n",
        "    print(\"    Filled missing values with median.\")\n",
        "\n",
        "# Class distribution plot (simple + useful)\n",
        "class_counts = pd.Series(y).value_counts().sort_index()\n",
        "fig, ax = plt.subplots(figsize=(9, 5))\n",
        "ax.bar(class_counts.index.astype(str), class_counts.values, edgecolor=\"black\")\n",
        "ax.set_title(\"Class Distribution (Dry Bean)\")\n",
        "ax.set_xlabel(\"Class\")\n",
        "ax.set_ylabel(\"Count\")\n",
        "plt.xticks(rotation=25, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"class_distribution.png\"), dpi=150)\n",
        "plt.close()\n",
        "print(\"    Saved: class_distribution.png\")\n",
        "\n",
        "# ============================================================\n",
        "# 3. Encode labels + Train/Test split\n",
        "# ============================================================\n",
        "print(\"\\n[3] Preprocessing...\")\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "class_names = le.classes_\n",
        "print(f\"    Encoded classes: {dict(zip(class_names, range(len(class_names))))}\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_encoded,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=y_encoded\n",
        ")\n",
        "print(f\"    Train: {X_train.shape[0]} samples\")\n",
        "print(f\"    Test:  {X_test.shape[0]} samples\")\n",
        "\n",
        "# ============================================================\n",
        "# 4. Models (3 only) + Cross-validation (NO leakage)\n",
        "# ============================================================\n",
        "print(\"\\n[4] Training + Cross-validation (no leakage via Pipeline)...\")\n",
        "\n",
        "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "models = {\n",
        "    \"Logistic Regression\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"clf\", LogisticRegression(max_iter=3000, random_state=RANDOM_STATE))\n",
        "    ]),\n",
        "    \"SVM (RBF)\": Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"clf\", SVC(kernel=\"rbf\", C=10, gamma=\"scale\", random_state=RANDOM_STATE))\n",
        "    ]),\n",
        "    \"Random Forest\": RandomForestClassifier(\n",
        "        n_estimators=300,\n",
        "        random_state=RANDOM_STATE,\n",
        "        n_jobs=-1\n",
        "    ),\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n  Model: {name}\")\n",
        "\n",
        "    # CV score on TRAIN ONLY (macro-F1 is better for multi-class fairness)\n",
        "    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring=\"f1_macro\")\n",
        "\n",
        "    # Fit on full training set, then evaluate on test set ONCE\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1_macro = f1_score(y_test, y_pred, average=\"macro\")\n",
        "    f1_weighted = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    prec_macro = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
        "    rec_macro = recall_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
        "\n",
        "    results[name] = {\n",
        "        \"cv_f1_macro_mean\": float(cv_scores.mean()),\n",
        "        \"cv_f1_macro_std\": float(cv_scores.std()),\n",
        "        \"test_accuracy\": float(acc),\n",
        "        \"test_f1_macro\": float(f1_macro),\n",
        "        \"test_f1_weighted\": float(f1_weighted),\n",
        "        \"test_precision_macro\": float(prec_macro),\n",
        "        \"test_recall_macro\": float(rec_macro),\n",
        "        \"y_pred\": y_pred,\n",
        "        \"model\": model\n",
        "    }\n",
        "\n",
        "    print(f\"    CV macro-F1: {cv_scores.mean():.4f} (+/- {cv_scores.std():.4f})\")\n",
        "    print(f\"    Test Acc:   {acc:.4f}\")\n",
        "    print(f\"    Test F1 (macro):    {f1_macro:.4f}\")\n",
        "    print(f\"    Test F1 (weighted): {f1_weighted:.4f}\")\n",
        "\n",
        "# Choose best model by CV macro-F1 (more correct than choosing by test)\n",
        "best_model_name = max(results, key=lambda k: results[k][\"cv_f1_macro_mean\"])\n",
        "best = results[best_model_name]\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"BEST MODEL (by CV macro-F1): {best_model_name}\")\n",
        "print(f\"CV macro-F1: {best['cv_f1_macro_mean']:.4f}\")\n",
        "print(f\"Test Acc:    {best['test_accuracy']:.4f}\")\n",
        "print(f\"Test macro-F1: {best['test_f1_macro']:.4f}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================\n",
        "# 5. Save confusion matrix + classification report (best model)\n",
        "# ============================================================\n",
        "print(\"\\n[5] Saving best model evaluation outputs...\")\n",
        "\n",
        "cm = confusion_matrix(y_test, best[\"y_pred\"])\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "im = ax.imshow(cm)\n",
        "ax.set_title(f\"Confusion Matrix - {best_model_name}\")\n",
        "ax.set_xlabel(\"Predicted\")\n",
        "ax.set_ylabel(\"Actual\")\n",
        "ax.set_xticks(range(len(class_names)))\n",
        "ax.set_yticks(range(len(class_names)))\n",
        "ax.set_xticklabels(class_names, rotation=30, ha=\"right\")\n",
        "ax.set_yticklabels(class_names)\n",
        "\n",
        "# annotate cells\n",
        "for i in range(cm.shape[0]):\n",
        "    for j in range(cm.shape[1]):\n",
        "        ax.text(j, i, str(cm[i, j]), ha=\"center\", va=\"center\", fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"confusion_matrix_best.png\"), dpi=150)\n",
        "plt.close()\n",
        "print(\"    Saved: confusion_matrix_best.png\")\n",
        "\n",
        "report = classification_report(y_test, best[\"y_pred\"], target_names=class_names, zero_division=0)\n",
        "with open(os.path.join(OUTPUT_DIR, \"classification_report_best.txt\"), \"w\") as f:\n",
        "    f.write(f\"Best Model (by CV macro-F1): {best_model_name}\\n\")\n",
        "    f.write(f\"CV macro-F1 mean: {best['cv_f1_macro_mean']:.4f} (std {best['cv_f1_macro_std']:.4f})\\n\")\n",
        "    f.write(f\"Test Accuracy: {best['test_accuracy']:.4f}\\n\")\n",
        "    f.write(f\"Test macro-F1: {best['test_f1_macro']:.4f}\\n\")\n",
        "    f.write(f\"Test weighted-F1: {best['test_f1_weighted']:.4f}\\n\\n\")\n",
        "    f.write(report)\n",
        "print(\"    Saved: classification_report_best.txt\")\n",
        "\n",
        "# ============================================================\n",
        "# 6. Save results table CSV\n",
        "# ============================================================\n",
        "print(\"\\n[6] Saving results table...\")\n",
        "\n",
        "summary_df = pd.DataFrame([\n",
        "    {\n",
        "        \"Model\": name,\n",
        "        \"CV_MacroF1_Mean\": r[\"cv_f1_macro_mean\"],\n",
        "        \"CV_MacroF1_Std\": r[\"cv_f1_macro_std\"],\n",
        "        \"Test_Accuracy\": r[\"test_accuracy\"],\n",
        "        \"Test_Precision_Macro\": r[\"test_precision_macro\"],\n",
        "        \"Test_Recall_Macro\": r[\"test_recall_macro\"],\n",
        "        \"Test_F1_Macro\": r[\"test_f1_macro\"],\n",
        "        \"Test_F1_Weighted\": r[\"test_f1_weighted\"],\n",
        "    }\n",
        "    for name, r in results.items()\n",
        "])\n",
        "\n",
        "summary_csv_path = os.path.join(OUTPUT_DIR, \"results_summary_3models.csv\")\n",
        "summary_df.to_csv(summary_csv_path, index=False)\n",
        "print(f\"    Saved: results_summary_3models.csv\")\n",
        "\n",
        "print(f\"\\nAll outputs saved to: {OUTPUT_DIR}\")\n",
        "print(\"Done!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 2b boxplot of top features by class\n",
        "\n",
        "print(\"\\n[2b] Creating feature boxplot...\")\n",
        "\n",
        "#convert to dataframe for easier plotting\n",
        "df_X = pd.DataFrame(X)\n",
        "df_X['Class'] = y\n",
        "\n",
        "#select a few key features to avoid overcrowding\n",
        "#choose features from different categories\n",
        "selected_features = ['Area', 'Perimeter', 'MajorAxisLength', 'MinorAxisLength',\n",
        "                     'AspectRatio', 'Eccentricity', 'Roundness', 'Compactness']\n",
        "\n",
        "#check which features actually exist in the dataset\n",
        "available_features = [f for f in selected_features if f in df_X.columns]\n",
        "if len(available_features) < len(selected_features):\n",
        "    #if not matching, use first 8 columns\n",
        "    available_features = df_X.columns[:8]\n",
        "\n",
        "#create boxplot\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 10))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, feature in enumerate(available_features[:8]):  #8 features limitation\n",
        "    df_X.boxplot(column=feature, by='Class', ax=axes[idx])\n",
        "    axes[idx].set_title(f'{feature} by Class')\n",
        "    axes[idx].set_xlabel('Bean Class')\n",
        "    axes[idx].tick_params(axis='x', rotation=45)\n",
        "\n",
        "plt.suptitle('Feature Distributions Across Bean Classes', fontsize=16)\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(OUTPUT_DIR, \"feature_boxplots.png\"), dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"    Saved: feature_boxplots.png\")\n",
        "\n",
        "#alternative: single large boxplot of standardized features\n",
        "print(\"\\n[2c] Creating standardized feature boxplot...\")\n",
        "\n",
        "#standardize features for comparison\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "df_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\n",
        "df_scaled['Class'] = y\n",
        "\n",
        "#melt dataframe for seaborn (if available)\n",
        "try:\n",
        "    import seaborn as sns\n",
        "    df_melted = df_scaled.melt(id_vars=['Class'], var_name='Feature', value_name='Value')\n",
        "\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.boxplot(data=df_melted, x='Feature', y='Value', hue='Class')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.title('Standardized Features Distribution by Class')\n",
        "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, \"standardized_boxplots.png\"), dpi=150, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"    Saved: standardized_boxplots.png\")\n",
        "except ImportError:\n",
        "    print(\"    Seaborn not available, skipping standardized boxplot\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZrlszG_5YEN",
        "outputId": "d2934cb6-7fd7-42ae-8ccb-02388041739a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[2b] Creating feature boxplot...\n",
            "    Saved: feature_boxplots.png\n",
            "\n",
            "[2c] Creating standardized feature boxplot...\n",
            "    Saved: standardized_boxplots.png\n"
          ]
        }
      ]
    }
  ]
}